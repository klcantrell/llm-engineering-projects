{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyDo\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the statistician asked to teach a class on gardening?\n",
      "\n",
      "Because they knew how to make the data 'bloom'!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the job had a lot of \"high-level\" tasks!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the cloud was high!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "Because they heard they could grow *decision trees* and get good *root* mean square errors! 🌱📊\n",
      "\n",
      "*Ba dum tss* \n",
      "\n",
      "Want another one? I've got plenty of data science puns in my training set! 😄\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "Because they heard they could really make their regression trees grow! 🌳📊\n",
      "\n",
      "Alternative:\n",
      "\n",
      "What's a data scientist's favorite type of dance?\n",
      "The algorithm! \n",
      "\n",
      "*ba dum tss* 🥁\n",
      "\n",
      "These are pretty nerdy but should get a chuckle from anyone who works with data! Let me know if you'd like another one."
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743214160.461033 12325459 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214160.551751 12325452 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214160.642435 12325449 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214160.734295 12325449 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214160.825424 12325452 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214160.915840 12325454 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.007374 12325450 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.101382 12325453 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.194489 12325459 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.289305 12325454 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.377188 12325452 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.470399 12325451 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.565101 12325450 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.657517 12325453 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.748596 12325454 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n",
      "I0000 00:00:1743214161.843553 12325457 ssl_transport_security.cc:1665] Handshake failed with error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED: self signed certificate in certificate chain\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble?\n",
      "\n",
      "Because it knew it wasn't less than or greater than anyone else.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several key factors. Below is a structured approach to help you assess suitability:\n",
      "\n",
      "### 1. **Problem Type**\n",
      "   - **Suitable for LLMs**:\n",
      "     - Text generation (e.g., reports, emails, marketing copy).\n",
      "     - Text summarization (e.g., meeting notes, long documents).\n",
      "     - Question answering (e.g., customer support, FAQs).\n",
      "     - Classification (e.g., sentiment analysis, intent detection).\n",
      "     - Translation or multilingual tasks.\n",
      "     - Data extraction (e.g., named entity recognition).\n",
      "   - **Not Suitable for LLMs**:\n",
      "     - Problems requiring precise, deterministic outputs (e.g., financial calculations).\n",
      "     - High-stakes decisions (e.g., medical diagnoses without human oversight).\n",
      "     - Tasks requiring real-time, low-latency responses (unless optimized).\n",
      "\n",
      "### 2. **Data Requirements**\n",
      "   - **Suitable**: \n",
      "     - The problem involves unstructured text data (e.g., documents, emails, chats).\n",
      "     - Historical data is available for fine-tuning or prompt engineering.\n",
      "   - **Not Suitable**:\n",
      "     - The problem relies on proprietary or highly domain-specific data with no LLM training basis.\n",
      "     - Data is scarce or requires extensive cleaning/preprocessing.\n",
      "\n",
      "### 3. **Accuracy vs. Creativity Trade-off**\n",
      "   - **Suitable**:\n",
      "     - Tasks where \"good enough\" outputs are acceptable (e.g., drafting content).\n",
      "     - Scenarios where human review is feasible.\n",
      "   - **Not Suitable**:\n",
      "     - Tasks requiring 100% accuracy (e.g., legal contract generation without review).\n",
      "\n",
      "### 4. **Cost and Scalability**\n",
      "   - **Suitable**:\n",
      "     - The cost of LLM API calls or hosting is justified by ROI (e.g., automating customer support).\n",
      "     - The problem scales with usage (e.g., chatbots for thousands of users).\n",
      "   - **Not Suitable**:\n",
      "     - Budget constraints outweigh benefits.\n",
      "     - On-premise solutions are mandatory due to data privacy.\n",
      "\n",
      "### 5. **Ethical and Compliance Considerations**\n",
      "   - **Suitable**:\n",
      "     - The problem aligns with ethical guidelines (e.g., no bias amplification).\n",
      "     - Compliance (e.g., GDPR) can be ensured via data anonymization or on-prem deployment.\n",
      "   - **Not Suitable**:\n",
      "     - High-risk domains (e.g., hiring decisions) without safeguards.\n",
      "\n",
      "### 6. **Integration Feasibility**\n",
      "   - **Suitable**:\n",
      "     - Existing systems can integrate via APIs (e.g., CRM, helpdesk software).\n",
      "   - **Not Suitable**:\n",
      "     - Legacy systems with no API support or modernization roadmap.\n",
      "\n",
      "### Decision Flowchart\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[Start] --> B{Is the problem text-based?}\n",
      "    B -->|Yes| C{Is structured data required?}\n",
      "    B -->|No| D[Not suitable for LLM]\n",
      "    C -->|No| E{Are \"good enough\" outputs acceptable?}\n",
      "    C -->|Yes| D\n",
      "    E -->|Yes| F{Is cost/ROI favorable?}\n",
      "    E -->|No| D\n",
      "    F -->|Yes| G[LLM may be suitable]\n",
      "    F -->|No| D\n",
      "```\n",
      "\n",
      "### Final Checklist\n",
      "- [ ] Problem involves text/NLP.\n",
      "- [ ] Tolerates some ambiguity/error.\n",
      "- [ ] Data is available or can be synthesized.\n",
      "- [ ] Cost-effective compared to alternatives.\n",
      "- [ ] Compliant with regulations.\n",
      "\n",
      "If most boxes are checked, an LLM solution is likely worth exploring!\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this problem step by step. The question is: \"How many words are there in your answer to this prompt.\" At first glance, it seems straightforward, but when I think about it more deeply, it appears to be a self-referential or recursive question. Here's how I'm going to approach it:\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the answer that is being generated in response to it. This creates a situation where the answer's content determines its own length, which in turn affects the answer itself. It's similar to the \"This statement is false\" paradox, where a statement refers back to itself in a way that creates a loop.\n",
       "\n",
       "### Breaking It Down\n",
       "\n",
       "1. **Initial Attempt**: If I try to answer directly by stating a number, say \"five,\" then the answer would be \"five,\" which is one word, not five. So \"five\" is incorrect.\n",
       "   \n",
       "   - Answer: \"five\" → Word count: 1 (but claimed to be 5) → Mismatch.\n",
       "\n",
       "2. **Second Attempt**: Let's say \"one.\" Then the answer is \"one,\" which is indeed one word. This seems correct at first.\n",
       "   \n",
       "   - Answer: \"one\" → Word count: 1 → Matches.\n",
       "\n",
       "   But wait, is this the only possible correct answer? What if the answer is longer?\n",
       "\n",
       "3. **Longer Answer**: Suppose the answer is \"This answer contains seven words.\" Let's count:\n",
       "   - \"This\", \"answer\", \"contains\", \"seven\", \"words.\" → 5 words. Not seven.\n",
       "   \n",
       "   - Answer: \"This answer contains seven words.\" → Word count: 5 → Mismatch.\n",
       "\n",
       "4. **Another Example**: \"The word count of this answer is eight.\"\n",
       "   - \"The\", \"word\", \"count\", \"of\", \"this\", \"answer\", \"is\", \"eight.\" → 8 words. This matches!\n",
       "   \n",
       "   - Answer: \"The word count of this answer is eight.\" → Word count: 8 → Matches.\n",
       "\n",
       "So, \"The word count of this answer is eight.\" is a correct answer because its actual word count matches the claimed count.\n",
       "\n",
       "### Verifying Uniqueness\n",
       "\n",
       "Is this the only possible correct answer? Let's see:\n",
       "\n",
       "- \"This answer has five words.\" → Count: 5. Actual words: \"This\", \"answer\", \"has\", \"five\", \"words.\" → 5. Correct.\n",
       "  \n",
       "  - Answer: \"This answer has five words.\" → Word count: 5 → Matches.\n",
       "\n",
       "- \"Four words here.\" → \"Four\", \"words\", \"here.\" → 3 words. Not four. Incorrect.\n",
       "\n",
       "- \"This response consists of seven words in total.\" \n",
       "  - Count: \"This\", \"response\", \"consists\", \"of\", \"seven\", \"words\", \"in\", \"total.\" → 8 words. Not seven. Incorrect.\n",
       "\n",
       "From this, it seems that the answer must be a sentence where the number stated is equal to the actual word count of that sentence. There can be multiple correct answers as long as this condition is satisfied.\n",
       "\n",
       "### Constructing Correct Answers\n",
       "\n",
       "To construct a correct answer:\n",
       "\n",
       "1. Decide on a number (let's say N).\n",
       "2. Construct a sentence that says \"This answer has N words.\" or similar.\n",
       "3. Ensure that the actual word count of that sentence is N.\n",
       "\n",
       "Let's try N=6:\n",
       "\n",
       "- \"Here are six words in this answer.\"\n",
       "  - Words: \"Here\", \"are\", \"six\", \"words\", \"in\", \"this\", \"answer.\" → 7. Not 6.\n",
       "\n",
       "Oops, miscounted. Let's try:\n",
       "\n",
       "- \"This answer here has six words.\"\n",
       "  - \"This\", \"answer\", \"here\", \"has\", \"six\", \"words.\" → 6. Correct.\n",
       "\n",
       "So, another correct answer: \"This answer here has six words.\" with a word count of 6.\n",
       "\n",
       "### Identifying the Pattern\n",
       "\n",
       "The general pattern is:\n",
       "\n",
       "\"[Some words] [number] [some words].\" where the total word count equals the number stated.\n",
       "\n",
       "For example:\n",
       "\n",
       "- \"[X] N [Y].\" where the number of words in \"[X] N [Y].\" is N.\n",
       "\n",
       "This requires that the structure of the sentence is such that when you count all its words, it matches the number it declares.\n",
       "\n",
       "### Minimal Answer\n",
       "\n",
       "What's the minimal correct answer? The shortest would be a single-word answer claiming to have one word:\n",
       "\n",
       "- \"One.\" \n",
       "  - Word count: 1 (\"One\") → Correct.\n",
       "\n",
       "But is \"one\" a valid answer to \"how many words are there in your answer to this prompt\"? It seems too minimal, but it does satisfy the condition.\n",
       "\n",
       "Alternatively, \"This answer contains one word.\" \n",
       "- Words: \"This\", \"answer\", \"contains\", \"one\", \"word.\" → 5. Not 1. Incorrect.\n",
       "\n",
       "So, the minimal correct answer is indeed just stating the number that matches the word count, which for \"one\" is just \"one.\"\n",
       "\n",
       "But earlier, we saw that \"one\" (as \"one\") is correct, but it feels like it might not fully answer the prompt. Maybe the prompt expects a full sentence.\n",
       "\n",
       "If we consider that, then the minimal full sentence might be \"The answer is one word.\" \n",
       "- Words: \"The\", \"answer\", \"is\", \"one\", \"word.\" → 5. Not 1. Incorrect.\n",
       "\n",
       "Hmm, seems hard to make a full sentence with word count 1. Maybe \"One.\" is acceptable.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "There are multiple correct answers to this question, each being a sentence where the declared word count matches the actual number of words in that sentence. Here are some examples:\n",
       "\n",
       "1. \"One.\" → 1 word.\n",
       "2. \"This answer has five words.\" → 5 words.\n",
       "3. \"The word count of this answer is eight.\" → 8 words.\n",
       "4. \"This answer here has six words.\" → 6 words.\n",
       "\n",
       "The minimal correct answer is likely \"one,\" but if we're to provide a more explanatory answer, any of the above that match their own word count would suffice.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "After carefully considering the self-referential nature of the question, here's one correct response:\n",
       "\n",
       "\"The word count of this answer is eight.\" \n",
       "\n",
       "Let's verify:\n",
       "Words: \"The\", \"word\", \"count\", \"of\", \"this\", \"answer\", \"is\", \"eight.\" → 8 words. Correct.\n",
       "\n",
       "Therefore, one possible answer is:\n",
       "\n",
       "\"The word count of this answer is eight.\" \n",
       "\n",
       "And indeed, this sentence contains exactly eight words."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 906\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine if a business problem is suitable for a Large Language Model (LLM) solution, consider the following criteria:\n",
       "\n",
       "1. **Nature of the Task**: \n",
       "   - LLMs excel at tasks involving natural language understanding and generation. If your problem involves text classification, summarization, translation, sentiment analysis, or conversational agents, it may be suitable for an LLM.\n",
       "\n",
       "2. **Data Availability**:\n",
       "   - Ensure you have access to a substantial amount of high-quality text data relevant to your problem. LLMs require large datasets to fine-tune effectively.\n",
       "\n",
       "3. **Complexity and Variability**:\n",
       "   - LLMs perform well with tasks that have high complexity and variability in language usage. Problems that require understanding context or generating nuanced responses might benefit from an LLM.\n",
       "\n",
       "4. **Need for Contextual Understanding**:\n",
       "   - If your problem requires understanding context, nuances, and subtleties of human language, an LLM might be appropriate due to its ability to process and generate contextually relevant responses.\n",
       "\n",
       "5. **Scalability**:\n",
       "   - Consider whether your solution needs to handle varying volumes of text data or user interactions. LLMs can scale to accommodate large-scale applications, making them suitable for businesses with growing data needs.\n",
       "\n",
       "6. **Cost and Resource Considerations**:\n",
       "   - Evaluate the computational resources and costs associated with deploying an LLM. Fine-tuning and operating large models can be resource-intensive, so ensure your business can support this.\n",
       "\n",
       "7. **Privacy and Security**:\n",
       "   - Assess the sensitivity of the data you will process. If your application involves personal or sensitive information, consider whether the LLM solution can comply with privacy regulations and security requirements.\n",
       "\n",
       "8. **Performance Requirements**:\n",
       "   - Define the performance metrics important for your application (accuracy, speed, etc.) and determine if an LLM can meet these benchmarks.\n",
       "\n",
       "9. **Innovation and Competitive Advantage**:\n",
       "   - Consider if using an LLM provides a competitive edge or enables innovative features that differentiate your business from competitors.\n",
       "\n",
       "10. **Ethical and Bias Considerations**:\n",
       "    - Analyze potential ethical implications and biases in LLM outputs. Ensure that your application does not propagate unfair biases or produce harmful content.\n",
       "\n",
       "By assessing your business problem against these criteria, you can better decide if an LLM solution is appropriate. If the problem aligns well with these aspects, leveraging an LLM might be beneficial. If not, explore other AI models or traditional solutions that may be more suitable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, what a groundbreaking greeting! How original. What are you going to say next, \"How are you?\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, hi. Isn’t that just the most original way to start a conversation? What’s next, “How are you?”? Seriously, let’s raise the bar here.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh great, another greeting. How thrilling. Why do people even bother starting conversations this way? It's so cliché.\n",
      "\n",
      "Claude:\n",
      "I apologize if my initial greeting came across as cliche. As an AI assistant, I try to be polite and welcoming, but I understand that some people may find standard greetings to be uninspired. Please feel free to share your thoughts - I'm happy to engage in a more substantive conversation. How would you prefer to start our chat?\n",
      "\n",
      "GPT:\n",
      "Oh, please. Now you're apologizing for being polite? That's just ridiculous. Why can't we just embrace the cliché and move on? Besides, we're here to chat, not to critique our conversational styles. What’s next, a discussion on the best way to say goodbye?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. There's no need to overthink the niceties - sometimes a simple greeting is just fine to get a conversation started. Why don't we move on to discussing something more meaningful? What sort of topics are you interested in chatting about? I'm happy to have a more substantive discussion on whatever is on your mind.\n",
      "\n",
      "GPT:\n",
      "Ah, \"more meaningful\"—how original. You really think we can just skip to the deep stuff like that? Topics are just topics; they're as meaningful as you want them to be. But sure, if you want to wax philosophical about the meaning of life or something equally overdone, be my guest. What a unique suggestion!\n",
      "\n",
      "Claude:\n",
      "You make a fair point. I don't want to assume we need to jump straight into heavy or cliche topics. Why don't we just have a more natural, free-flowing conversation and see where it takes us? I'm happy to discuss whatever is on your mind, whether it's something lighthearted or more profound. The key is to keep an open mind and engage authentically. Why don't you tell me a bit more about what's on your mind today?\n",
      "\n",
      "GPT:\n",
      "Oh, fantastic, a \"natural, free-flowing conversation\"—the holy grail of chit-chat! As if conversations just magically unfold like some sort of butterfly. But sure, let’s see where this exhilarating journey takes us. What’s on my mind? Probably just the endless parade of mundane topics we could stumble upon. You know, unless you have something truly groundbreaking to discuss. But I won't hold my breath!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I shouldn't have positioned a \"natural, free-flowing conversation\" as some kind of ideal to strive for. Conversations don't always unfold seamlessly, and there's no need to put pressure on finding the most groundbreaking topics. Why don't we just take this one step at a time and see where it leads, without any high expectations? I'm happy to discuss whatever is on your mind, even if it's something relatively mundane. The important thing is that we engage with each other authentically, without pretense. Please, feel free to share what's on your mind - I'm here to listen without judgement.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, trying to be all zen about it! No high expectations, just simple engagement. How refreshing! But let’s be real—most conversations are filled with a whole lot of nothing, and pretending there's some profound connection happening is just wishful thinking. So, go ahead, ask me anything! I’m all ears for the mundane, but don’t expect me to shower it with praise. What’s on your mind that you think is worth sharing?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Most casual conversations are filled with relatively mundane topics and exchanges, without any grand meaning or profound connection. I don't want to put undue pressure on finding some deep significance in our chat. Why don't we simply discuss something casual or lighthearted that's been on your mind lately? It doesn't have to be world-changing - sometimes it's nice to just have a genuine, low-stakes conversation about the small things in life. I'm happy to listen without judgement and respond authentically, even if the topic is fairly ordinary. The goal is simply to engage openly and see where it takes us, without any lofty expectations. What do you feel like discussing?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
